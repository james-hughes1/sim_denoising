\documentclass[12pt]{article}

\title{Data Analysis Project Report}
\author{James Hughes}

\usepackage[nottoc,numbib]{tocbibind}
\usepackage{graphicx}
\usepackage[export]{adjustbox}

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}

        \Huge
        \textbf{Structured Illumination Microscopy Image Processing using Deep Learning}

        \vspace{0.5cm}
        \LARGE

        James Hughes

        Supervised by Dr Edward Ward

        \vspace{2cm}
        \Huge
        \textbf{Executive Summary}

        \vfill

        MPhil, Data Intensive Science

        \vspace{0.8cm}

        \Large
        Department of Physics \& Department of Chemical Engineering and Biotechnology

        University of Cambridge

        United Kingdom

        28th June 2024

    \end{center}
\end{titlepage}

\newpage

\newpage
\section{Introduction}

This project represents the culmination of my studies in Data Intensive Science,
and is a perfect example of what Data Intenstive Science could actually refer to.
The focus of the project was a very recent piece of research,
which developed a deep-learning based method to improve the images generated by a microscopy technique known as Structured Illumination Microscopy (SIM).
As such, the work involved a mixture of skills in software development, deep learning, data analysis, and domain-specific knowledge.
I have worked on the first three sets of skills over the course of my education and professional experience.
On the other hand, I had very little prior understanding of the application domain,
and so a key challenge of the project was quickly learning the necessary fundamentals of microscopy,
and later the specific underpinnings of the SIM technique.

The main objectives of the work were two-fold:
\begin{enumerate}
    \item develop software implementing the full data processing pipeline that enables the new method to be used, and,
    \item investigate the reproducibility of the research by applying it to new datasets.
\end{enumerate}

\section{Technical Details}

\begin{figure}[hbtp]
    \includegraphics[scale=0.3, center]{moire.png}
    \caption{Moir\'{e} Fringes}
    \label{fig:moire}
\end{figure}

In microscopy, resolution refers approximately to the smallest possible distance between two objects which appear as distinct features in the resulting image---better resolution means finer details are visible.
Widefield microscopy is traditionally limited by the so-called Abbe diffraction limit.
The SIM technique manages to surpass this limit by a factor of two by exploiting the `Moir\'e effect',
illustrated in Figure \ref{fig:moire}.
Crucially, when this image is viewed from a sufficient distance,
the fine details from both of the striped patterns `blur together' and cannot be resolved.
However, the thick vertical fringe pattern that they produce can be resolved even at a distance.
SIM similarly uses a fine striped pattern to illuminate the sample,
which interferes with the complex spatial patterns that we are trying to view in the sample.
The result---analogous to the Moir\'e fringes---can then be analysed,
along with knowledge of the illumination pattern,
to reverse engineer finer details in the original image than could be perceived in widefield microscopy.

When imaging time-lapses of biological processes using SIM,
a key issue is phototoxicity effects,
which causes the cell structures to be damaged as a result of the imaging process.
Li et al.'s two-step denoising method \cite{keypaper} addresses this by taking images using a much lower illumination intensity.
This increases the duration of imaging before damage is incurred, but reduces the signal-to-noise ratio of the images.
In turn, the method compensates for this by denoising the acquired images, applying the SIM reconstruction, and denoising again.
Both denoising steps are performed by `residual channel attention network' (RCAN) models \cite{keypaper}.

While open-source software for the RCAN model and SIM reconstruction was available,
the RCAN software needed to be migrated to PyTorch in order to enable compatibility with the Cambridge computing cluster service (CSD3).
Moreover, these needed to be unified into a single pipeline as part of the project.
Additionally, the original data used to train the models was unavailable,
meaning that new datasets needed to be sourced.
In the first instance, I learnt how to use a real 2D SIM system to acquire images of two cell structures: microtubules and the endoplasmic reticulum.
Later, I created a synthetic dataset simulating the images acquired from a 3D SIM microscope,
which is where the original method was applied.

\section{Outcomes}

The final data processing pipeline has many steps,
and a pair of models takes a long time to fully develop.
Indeed, the final trained models were trained for 36 hours each,
and the SIM reconstruction process in the middle of the development pipeline can take many hours to process (at least for the 3D data).
Building such a time-intensive pipeline, as well as using the queueing system for CSD3, required disciplined time-management.
It was also necessary to use best practices of software development, producing code that was well-documented and modular.
Good version control and organisation of data and I/O was crucial since the code existed on my laptop for development purposes,
but was often being run on CSD3, training multiple models simultaneously.
This enabled me to improve my programming skills, as applied to large projects.
I developed crucial core skills during certain tasks for the project, namely:
quickly coming to grips with a completely unknown technical subject,
reading scientific literature,
compiling the main analysis report,
and delivering multiple presentations about the project to my supervisor's research group in the Chemical Engineering and Biotechnology department.

Ultimately, the full pipeline was developed and applied to multiple different datasets.
The project corroborated many of the claims of the original research.
However, it also uncovered concerning unintended effects of the method for those wishing to apply it to new datasets.
The way in which the software was created means that it should be fairly easy for the code to be inspected,
used, or even extended, by other researchers.

\bibliographystyle{IEEEtran}
\bibliography{Biblio}

\end{document}
