\documentclass[12pt]{article}

\title{Data Analysis Project Report}
\author{James Hughes}

\usepackage[nottoc,numbib]{tocbibind}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{url}
\usepackage[hidelinks]{hyperref}

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}

        \Huge
        \textbf{Structured Illumination Microscopy Image Processing using Deep Learning}

        \vspace{0.5cm}
        \LARGE

        James Hughes

        Supervised by Dr Edward Ward

        \vspace{2cm}
        \Huge
        \textbf{Executive Summary}

        \vfill

        MPhil, Data Intensive Science

        \vspace{0.8cm}

        \Large
        Department of Physics \& Department of Chemical Engineering and Biotechnology

        University of Cambridge

        United Kingdom

        28th June 2024

        (Word count: 869)

    \end{center}
\end{titlepage}

\newpage

\newpage
\section*{Introduction}

This project represents the culmination of my studies in Data Intensive Science.
The focus of the project was a very recent piece of research,
which developed a deep-learning based method to improve the images generated by a microscopy technique known as structured illumination microscopy (SIM).
As such, the work involved a mixture of skills in software development, deep learning, data analysis, and domain-specific knowledge.
I have worked on the first three sets of skills over the course of my education and professional experience.
On the other hand, I had very little prior understanding of the application domain,
and so a key challenge of the project was quickly learning the necessary fundamentals of microscopy,
and later understanding the theory underpinning the SIM technique.

The main objectives of the work were two-fold:
\begin{enumerate}
    \item develop software implementing the full data processing pipeline required for the proposed method, and,
    \item investigate the reproducibility of the research by applying it to new datasets.
\end{enumerate}

\section*{Technical Details}

The central focus of the project was the SIM technique, which was developed over two decades ago
as a way of improving the resolution of microscope images \cite{SIM2000}.
In microscopy, resolution refers approximately to the smallest possible distance between two objects which appear as distinct features in the resulting image---better resolution means finer details are visible.
In widefield microscopy this distance is traditionally limited by the so-called `Abbe diffraction limit' \cite{abbe}.
The SIM technique surpasses this limit by a factor of two by exploiting the `Moir\'e effect' \cite{SIM2000},
illustrated in Figure \ref{fig:moire}.
Crucially, when Figure \ref{fig:moire} is viewed from a sufficient distance,
the fine details from both of the striped patterns `blur together' and cannot be individually resolved by the observer.
However, the thick vertical Moir\'e fringe pattern produced by interference can be resolved even at a distance.
In a similar way, SIM uses a fine striped pattern to illuminate the sample,
which interferes with the complex spatial patterns that exist within the structures present in the sample.
The resulting image---analogous to the Moir\'e fringes---can then be reverse-engineered
to reveal finer ground-truth details than could be perceived by widefield microscopy.
Typically nine or fifteen SIM images are acquired, and combined, using a reconstruction algorithm to achieve a single, super-resolution image.

\begin{figure}[tbp]
    \includegraphics[scale=0.3, center]{moire.png}
    \caption{Moir\'{e} Fringes}
    \label{fig:moire}
\end{figure}

It is often desirable to capture a time-lapse of a biological process that is being investigated.
This can be achieved by marking specific targets within the cell using a fluorophore,
and then performing continuous SIM imaging, using a laser at the correct wavelength to cause fluorescence.
A key problem with this approach is the problematic effects of phototoxicity,
which includes damage to cell structures due to the release of reactive oxygen species from the fluorophore \cite{phototoxicity}.
Li et al.'s two-step denoising method \cite{keypaper} addresses this by taking images using a much lower illumination intensity.
This increases the duration of imaging that can be performed before cell damage occurs, but reduces the signal-to-noise ratio of the images.
In turn, the method compensates for this by denoising the acquired images, applying the SIM reconstruction, and denoising again.
Both denoising steps are performed by `residual channel attention network' (RCAN) models \cite{keypaper}.

In this work, this data processing pipeline was reproduced, and the results analysed.
Although open-source software for the RCAN model and SIM reconstruction was available,
the RCAN software had to be migrated to PyTorch in order to enable compatibility with the Cambridge Service for Data-Driven Discovery (CSD3),
which facilitated training the deep learning models using modern GPUs.
Additionally, the original data used to train the models was unavailable,
meaning that new datasets needed to be sourced.
In the first instance, I learnt how to use a real 2D SIM system to acquire images in which two cell structures were labelled: microtubules and the endoplasmic reticulum.
Later, I created a synthetic dataset simulating the images acquired from a 3D SIM microscope,
which is where the original method was applied.

\section*{Outcomes}

The final data processing pipeline has many steps,
and a pair of models takes a long time to fully develop.
Indeed, the final models were trained for 36 hours each,
and the SIM reconstruction process in the middle of the development pipeline can take many hours to process (at least for the 3D data).
Building such a time-intensive pipeline, as well as using the queueing system for CSD3, required disciplined time-management.
It was also necessary to use best practices of software development, producing code that was well-documented and modular.
Good version control and organisation of data and I/O operations was crucial since the code existed on my laptop for development purposes,
but was often being run on CSD3, training multiple models simultaneously.
This enabled me to improve my programming skills, and will be valuable experience when working on large software projects in the future.
I developed crucial core skills during certain tasks for the project, namely:
quickly familiarising myself with a completely unknown technical subject,
reading scientific literature,
compiling the main analysis report,
and delivering multiple presentations about the project to my supervisor's research group in the Chemical Engineering and Biotechnology department.

Ultimately, the full pipeline was developed and applied to multiple different datasets.
The project corroborated many of the claims of the original research.
However, it also uncovered concerning unintended effects of the method for those wishing to apply it to new datasets.
The way in which the software was created means that it should be fairly easy for the code to be inspected,
used, or even extended, by other researchers.

\newpage

\bibliographystyle{IEEEtran}
\bibliography{Biblio}

\end{document}
